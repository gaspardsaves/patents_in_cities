# Importation de biblioth√®ques
import pandas as pd
import os
import sqlite3

# Partie 1 : Donn√©es des brevets

# Contruction dela liste des fichiers de donn√©es
inputsRepertory = "inputs"
inputsFiles = [f for f in os.listdir(inputsRepertory) if f.endswith(".csv")]

dtypeData = {
    "patent_number": str,
    "grant_year": "Int64",
    "city": str,
    "state": str,
    "first_wipo_field_title": str,
    "first_wipo_sector_title": str
}

# Lire tous les fichiers et les stocker dans une liste de DataFrames
dataframes = [pd.read_csv(os.path.join(inputsRepertory, file), dtype=dtypeData, low_memory=False) for file in inputsFiles]
# Fusionner tous les DataFrames en un seul
df_InternationalPatentsUSA = pd.concat(dataframes, ignore_index=True)

# Tri et curage des donn√©es
# Filtrage pour conserver uniquement les brevets am√©ricains
df_PatentsUSA = df_InternationalPatentsUSA[df_InternationalPatentsUSA["country"] == "US"].copy()
# Suppression des colonnes inutiles pour l'analyse
df_PatentsUSA = df_PatentsUSA[[
    "patent_number", "grant_year",
    "city", "state", "first_wipo_field_title", "first_wipo_sector_title"
]]
# Suppression des brevets pour lesquels les donn√©es de localsation sont manquantes
df_PatentsUSA = df_PatentsUSA.dropna(subset=['state', 'city'])

# Sauvegarde des donn√©es nettoy√©es
# Dans un CSV
outputFile = "data/patentsUSA.csv"
df_PatentsUSA.to_csv(outputFile, index=False, encoding="utf-8")
print(f"üìÅ Fichier sauvegard√© : {outputFile}")

# Dans une base de donn√©es pour am√©liorer les performances
# Cr√©ation de la base de donn√©es SQL
dataBase = "data/patentsUSA.db"
connex = sqlite3.connect(dataBase)
# Sauvegarde des brevets dans une table 'patents'
df_PatentsUSA.to_sql("patents", connex, if_exists="replace", index=False)
print(f"üìÇ Base de donn√©es cr√©e avec succ√®s : {dataBase}")
connex.close()

# Partie 2 : Donn√©es des villes et donn√©es d√©mographiques